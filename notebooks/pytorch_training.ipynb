{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a074e8e6",
   "metadata": {},
   "source": [
    "1. Imports and Setup:\n",
    "\n",
    "Place all imports (e.g., torch, transformers, etc.) and environment setup (e.g., GPU configuration) at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19025fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Verify tensor placement\n",
    "x = torch.randn(3, 3).to(device)\n",
    "print(\"Tensor is on device:\", x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881427dc",
   "metadata": {},
   "source": [
    "2. Define the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16fa8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "label_to_id = {\"O\": 0, \"PERSON\": 1, \"ORG\": 2, \"PHONE\": 3, \"EMAIL\": 4}  # Example label mapping\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_to_id))\n",
    "model.to(device)\n",
    "\n",
    "# Verify model placement\n",
    "print(\"Model is on device:\", next(model.parameters()).device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d86236",
   "metadata": {},
   "source": [
    "3. Load Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0c425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import training_data\n",
    "\n",
    "# Reload the module to pick up changes\n",
    "importlib.reload(training_data)\n",
    "\n",
    "# Access the updated TRAIN_DATA\n",
    "TRAIN_DATA = training_data.TRAIN_DATA\n",
    "\n",
    "# Print the updated count\n",
    "print(\"Number of training examples:\", len(TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157a10a",
   "metadata": {},
   "source": [
    "4. Preprocess Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd2b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tokenizer (using a pre-trained tokenizer, e.g., BERT)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_data, val_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocess_data function\n",
    "def preprocess_data(data, tokenizer, label_to_id):\n",
    "    tokenized_data = []\n",
    "\n",
    "    # Debugging: Print tokenized input and offsets\n",
    "    for text, annotations in train_data[:5]:  # Check the first 5 examples\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        print(\"Text:\", text)\n",
    "        print(\"Tokenized Input IDs:\", tokenized[\"input_ids\"])\n",
    "        print(\"Offsets:\", tokenized[\"offset_mapping\"])\n",
    "        print(\"Annotations:\", annotations)\n",
    "\n",
    "        # Initialize labels with \"O\" (outside any entity)\n",
    "        labels = [label_to_id[\"O\"]] * len(tokenized[\"input_ids\"][0])\n",
    "\n",
    "        # Align labels with tokens\n",
    "        offsets = tokenized[\"offset_mapping\"][0].tolist()\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            for idx, (token_start, token_end) in enumerate(offsets):\n",
    "                if token_start >= start and token_end <= end:\n",
    "                    labels[idx] = label_to_id[label]\n",
    "\n",
    "        # Remove offset mapping (not needed for training)\n",
    "        tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "        # Add labels to the tokenized data\n",
    "        tokenized[\"labels\"] = torch.tensor(labels)\n",
    "\n",
    "        tokenized_data.append(tokenized)\n",
    "\n",
    "    return tokenized_data\n",
    "\n",
    "# Preprocess training and validation data\n",
    "tokenized_train_data = preprocess_data(train_data, tokenizer, label_to_id)\n",
    "tokenized_val_data = preprocess_data(val_data, tokenizer, label_to_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc11a442",
   "metadata": {},
   "source": [
    "5. Create DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06030ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val.clone().detach() for key, val in self.data[idx].items()}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'].squeeze(0) for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'].squeeze(0) for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "# Create DataLoaders with the custom collate_fn\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85ee5a2",
   "metadata": {},
   "source": [
    "6. Train the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28065f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Gradient accumulation steps\n",
    "accumulation_steps = 4  # Simulate a larger batch size by accumulating gradients\n",
    "\n",
    "# Training loop with gradient accumulation\n",
    "for epoch in range(30):  # Train for 20 epochs instead of 10\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / accumulation_steps  # Scale loss by accumulation steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights after accumulating gradients\n",
    "        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}, Step {step + 1}, Loss: {loss.item()}\")\n",
    "        print(f\"Allocated memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"Reserved memory: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Validation loop\n",
    "model.eval()\n",
    "val_loss = 0\n",
    "for batch in val_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        val_loss += outputs.loss.item()\n",
    "\n",
    "val_loss /= len(val_loader)\n",
    "print(f\"Validation Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e4d5a",
   "metadata": {},
   "source": [
    "7. Save the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292afe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model with label mappings\n",
    "model.config.id2label = {v: k for k, v in label_to_id.items()}  # Map IDs to labels\n",
    "model.config.label2id = label_to_id  # Map labels to IDs\n",
    "\n",
    "model.save_pretrained(\"ner_model\")\n",
    "tokenizer.save_pretrained(\"ner_model\")\n",
    "print(\"Model saved to 'ner_model'\")\n",
    "\n",
    "entities = ner_pipeline(text)\n",
    "print(\"Raw Predictions:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a7823",
   "metadata": {},
   "source": [
    "8. Test the Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a60c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process predictions\n",
    "def post_process_predictions(entities):\n",
    "    merged_entities = []\n",
    "    temp_entity = None\n",
    "\n",
    "    for entity in entities:\n",
    "        # Remove subword tokens (e.g., \"##\")\n",
    "        entity[\"word\"] = entity[\"word\"].replace(\"##\", \"\")\n",
    "\n",
    "        if temp_entity and entity[\"entity\"] == temp_entity[\"entity\"] and entity[\"start\"] == temp_entity[\"end\"]:\n",
    "            # Merge consecutive tokens\n",
    "            temp_entity[\"word\"] += entity[\"word\"]\n",
    "            temp_entity[\"end\"] = entity[\"end\"]\n",
    "            temp_entity[\"score\"] = max(temp_entity[\"score\"], entity[\"score\"])\n",
    "        else:\n",
    "            if temp_entity:\n",
    "                merged_entities.append(temp_entity)\n",
    "            temp_entity = entity\n",
    "\n",
    "    if temp_entity:\n",
    "        merged_entities.append(temp_entity)\n",
    "\n",
    "    return merged_entities\n",
    "\n",
    "    # Remove duplicates and overlapping predictions\n",
    "    # unique_entities = []\n",
    "    # seen = set()\n",
    "    # for entity in merged_entities:\n",
    "    #     key = (entity[\"start\"], entity[\"end\"], entity[\"entity\"])\n",
    "    #     if key not in seen:\n",
    "    #         unique_entities.append(entity)\n",
    "    #         seen.add(key)\n",
    "\n",
    "    # return unique_entities\n",
    "\n",
    "# Test the model on additional examples\n",
    "texts = [\n",
    "    \"John Doe works at Acme Corp. His email is john.doe@acme.com and phone is 123-456-7890.\",\n",
    "    \"Reach out to Jane Smith at jane.smith@domain.org or call 987-654-3210.\",\n",
    "    \"Contact Alice Johnson at alice.johnson@example.com or 555-123-4567.\",\n",
    "    \"Michael Brown's phone number is 800-555-0199 and email is michael.brown@domain.com.\",\n",
    "    \"Sarah Connor works at Skynet. Her email is sarah.connor@skynet.com.\",\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    entities = ner_pipeline(text)\n",
    "    cleaned_entities = post_process_predictions(entities)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Entities: {cleaned_entities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13054d88",
   "metadata": {},
   "source": [
    "9. Save and Load JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325e2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Store results in a list\n",
    "results = []\n",
    "\n",
    "for text in texts:\n",
    "    entities = ner_pipeline(text)\n",
    "    cleaned_entities = post_process_predictions(entities)\n",
    "\n",
    "    # Convert np.float32 to float for JSON serialization\n",
    "    for entity in cleaned_entities:\n",
    "        entity[\"score\"] = float(entity[\"score\"])\n",
    "\n",
    "    results.append({\"text\": text, \"entities\": cleaned_entities})\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"ner_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"Results saved to ner_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
